{
  "root": "/Users/prabhatsingh/param-tatva-sim",
  "generated_at_utc": "2025-09-25T14:47:27.324350Z",
  "files": [
    {
      "path": "gen__kernel_anime.py",
      "size": 2000,
      "content": "import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nimport json\n\n# Re-load kernel\nwith open(\"ptk.v1.json\", \"r\", encoding=\"utf-8\") as f:\n    ptk = json.load(f)\n\nwith open(\"ptk.layout.json\", \"r\", encoding=\"utf-8\") as f:\n    layout = json.load(f)\n\ncoords = layout[\"coords\"]\nlabels = {n[\"id\"]: n[\"label\"] for n in ptk[\"nodes\"]}\n\n# Precompute edge lines\nedges_pos, edges_neg = [], []\nfor e in ptk[\"edges\"]:\n    src, dst = e[\"source\"], e[\"target\"]\n    if src not in coords or dst not in coords: \n        continue\n    x1,y1 = coords[src]\n    x2,y2 = coords[dst]\n    if e[\"polarity\"] == 1:\n        edges_pos.append(((x1,y1),(x2,y2)))\n    else:\n        edges_neg.append(((x1,y1),(x2,y2)))\n\nfig, ax = plt.subplots(figsize=(14,10))\nax.set_aspect('equal')\nax.axis('off')\n\n# Draw nodes\nfor nid,(x,y) in coords.items():\n    ax.plot(x,y,\"o\", color=\"white\", mec=\"black\", zorder=3)\n    ax.text(x,y,labels.get(nid,\"\"), fontsize=7, ha=\"center\", va=\"center\", zorder=4)\n\n# Collections for animated arrows\npos_lines = [ax.plot([],[], color=\"green\", lw=1, alpha=0.6)[0] for _ in edges_pos]\nneg_lines = [ax.plot([],[], color=\"red\", lw=1, alpha=0.6)[0] for _ in edges_neg]\n\ndef init():\n    for ln in pos_lines+neg_lines:\n        ln.set_data([],[])\n    return pos_lines+neg_lines\n\ndef animate(frame):\n    # animate along each edge with a moving dot or pulsing segment\n    frac = (frame % 30) / 30.0\n    for ln,(p1,p2) in zip(pos_lines, edges_pos):\n        x1,y1 = p1; x2,y2 = p2\n        xm, ym = x1 + frac*(x2-x1), y1 + frac*(y2-y1)\n        ln.set_data([x1,xm],[y1,ym])\n    for ln,(p1,p2) in zip(neg_lines, edges_neg):\n        x1,y1 = p1; x2,y2 = p2\n        xm, ym = x1 + frac*(x2-x1), y1 + frac*(y2-y1)\n        ln.set_data([x1,xm],[y1,ym])\n    return pos_lines+neg_lines\n\nani = animation.FuncAnimation(fig, animate, init_func=init,\n                              frames=60, interval=100, blit=True)\n\noutpath = \"ptk_overlay_flows.gif\"\nani.save(outpath, writer=\"pillow\", dpi=120)\noutpath\n",
      "sha256": "10d8944cec610a0ce93c8ef16228cc1de1079b29fa6f1eb0936fa739f9a4a4fb"
    },
    {
      "path": "gen__kernel_viz.py",
      "size": 2648,
      "content": "import os, json\nimport matplotlib.pyplot as plt\n\nres_path = \"out/ptk_emission_result.json\"\n\nres = json.load(open(res_path)) if os.path.exists(res_path) else None\n\n\n# Load kernel JSON\nwith open(\"ptk.v1.json\", \"r\", encoding=\"utf-8\") as f:\n    ptk = json.load(f)\n\ncoords = {}\n# Try to load layout too (pixel-perfect positions)\nlayout_path = \"ptk.layout.json\"\ntry:\n    with open(layout_path, \"r\", encoding=\"utf-8\") as f:\n        layout = json.load(f)\n    coords = layout[\"coords\"]\nexcept FileNotFoundError:\n    # fallback: simple layered layout by line\n    node_w, node_h = 70, 34\n    row_gap, col_gap = 85, 110\n    margin_x, margin_y = 40, 40\n    nodes_by_line = {}\n    for n in ptk[\"nodes\"]:\n        nodes_by_line.setdefault(n[\"line\"], []).append(n)\n    for line in nodes_by_line:\n        nodes_by_line[line].sort(key=lambda x: x[\"pos\"])\n        total_w = len(nodes_by_line[line])*node_w + (len(nodes_by_line[line])-1)*col_gap\n        start_x = margin_x\n        y = margin_y + (line-1)*row_gap\n        for idx, n in enumerate(nodes_by_line[line]):\n            x = start_x + idx*(node_w+col_gap)\n            coords[n[\"id\"]] = [x,y]\n\n# Build dict for labels\nlabels = {n[\"id\"]: n[\"label\"] for n in ptk[\"nodes\"]}\n\n# Draw\nfig, ax = plt.subplots(figsize=(14, 10))\nax.set_aspect('equal')\nax.axis('off')\n\n# Draw edges\nfor e in ptk[\"edges\"]:\n    src, dst = e[\"source\"], e[\"target\"]\n    if src not in coords or dst not in coords:\n        continue\n    x1,y1 = coords[src]\n    x2,y2 = coords[dst]\n    color = \"green\" if e[\"polarity\"]==1 else \"red\"\n    ax.annotate(\"\",\n                xy=(x2,y2), xycoords='data',\n                xytext=(x1,y1), textcoords='data',\n                arrowprops=dict(arrowstyle=\"->\", color=color,\n                                lw=0.8, alpha=0.6))\n\n# Draw nodes\nfor nid,(x,y) in coords.items():\n    ax.plot(x,y,\"o\", color=\"white\", mec=\"black\")\n    ax.text(x,y,labels.get(nid,\"\"), fontsize=8,\n            ha=\"center\", va=\"center\")\n\n\nif res:\n    # fields: highlight support edges\n    field_edge_ids = set(eid for f in res[\"fields\"] for eid in f[\"support_edges\"])\n    for e in ptk[\"edges\"]:\n        if e[\"id\"] in field_edge_ids:\n            x1,y1 = coords[e[\"source\"]]; x2,y2 = coords[e[\"target\"]]\n            ax.plot([x1,x2],[y1,y2], lw=2.4, alpha=0.35)  # thicker wash\n\n    # particles: draw star markers at locus nodes\n    for p in res[\"particles\"]:\n        x,y = coords[p[\"locus\"]]\n        ax.plot(x,y, marker=\"*\", ms=12, mec=\"black\", mfc=\"gold\", zorder=5)\n        \nplt.title(\"Param Tatva Kernel — Positive (green) & Negative (red) flows\")\nplt.tight_layout()\noutpath = \"ptk_overlay_flows.png\"\nplt.savefig(outpath, dpi=150)\noutpath\n",
      "sha256": "33d367642ab2ec571337df7d8f2988414e14fa447e65fbd7fec6286bdd6b32b9"
    },
    {
      "path": "generate_ptk_artifacts.py",
      "size": 11700,
      "content": "# generate_ptk_artifacts.py\n# Creates:\n#   - ptk.v1.json\n#   - ptk.layout.json\n#   - ptk_pydantic_validator.py\n#   - ptk_rust.rs\n#   - ptk_rust_tests.rs\n\nimport json, datetime, os, textwrap\n\n# --- 1) Canonical nodes (Maheshwara Sutras, order preserved) ---\nLINES = {\n    1:  [\"a\",\"i\",\"u\",\"Ṇ\"],\n    2:  [\"ṛ\",\"ḷ\",\"K\"],\n    3:  [\"e\",\"o\",\"Ṅ\"],\n    4:  [\"ai\",\"au\",\"C\"],\n    5:  [\"ha\",\"ya\",\"va\",\"ra\",\"Ṭ\"],\n    6:  [\"la\",\"Ṇ\"],\n    7:  [\"ña\",\"ma\",\"ṅa\",\"ṇa\",\"na\",\"M\"],\n    8:  [\"jha\",\"bha\",\"Ñ\"],\n    9:  [\"gha\",\"ḍha\",\"dha\",\"Ṣ\"],\n    10: [\"ja\",\"ba\",\"ga\",\"ḍa\",\"da\",\"Ś\"],\n    11: [\"kha\",\"pha\",\"cha\",\"ṭha\",\"tha\",\"ca\",\"ṭa\",\"ta\",\"V\"],\n    12: [\"ka\",\"pa\",\"Y\"],\n    13: [\"śa\",\"ṣa\",\"sa\",\"R\"],\n    14: [\"ha\",\"L\"],\n}\n\nDEV_MAP = {\n    \"a\":\"अ\",\"i\":\"इ\",\"u\":\"उ\",\"Ṇ\":\"ण्\",\n    \"ṛ\":\"ऋ\",\"ḷ\":\"ऌ\",\"K\":\"क्\",\n    \"e\":\"ए\",\"o\":\"ओ\",\"Ṅ\":\"ङ्\",\n    \"ai\":\"ऐ\",\"au\":\"औ\",\"C\":\"च्\",\n    \"ha\":\"ह\",\"ya\":\"य\",\"va\":\"व\",\"ra\":\"र\",\"Ṭ\":\"ट्\",\n    \"la\":\"ल\",\"ña\":\"ञ\",\"ma\":\"म\",\"ṅa\":\"ङ\",\"ṇa\":\"ण\",\"na\":\"न\",\"M\":\"म्\",\n    \"jha\":\"झ\",\"bha\":\"भ\",\"Ñ\":\"ञ्\",\n    \"gha\":\"घ\",\"ḍha\":\"ढ\",\"dha\":\"ध\",\"Ṣ\":\"ष्\",\n    \"ja\":\"ज\",\"ba\":\"ब\",\"ga\":\"ग\",\"ḍa\":\"ड\",\"da\":\"द\",\"Ś\":\"श्\",\n    \"kha\":\"ख\",\"pha\":\"फ\",\"cha\":\"छ\",\"ṭha\":\"ठ\",\"tha\":\"थ\",\"ca\":\"च\",\"ṭa\":\"ट\",\"ta\":\"त\",\"V\":\"व्\",\n    \"ka\":\"क\",\"pa\":\"प\",\"Y\":\"य्\",\n    \"śa\":\"श\",\"ṣa\":\"ष\",\"sa\":\"स\",\"R\":\"र्\",\n    \"L\":\"ळ\"\n}\n\ndef build_nodes():\n    nodes = []\n    nid = 1\n    for line, arr in LINES.items():\n        for pos, lab in enumerate(arr, start=1):\n            nodes.append({\n                \"id\": f\"n{nid}\",\n                \"label\": lab,\n                \"sanskrit\": DEV_MAP.get(lab),\n                \"line\": line,\n                \"pos\": pos,\n                \"features\": []\n            })\n            nid += 1\n    return nodes\n\n# --- 2) Edges: within-line ±, cross-sutra ±, special ha↔ha ± ---\nCROSS = [\n    (\"n4\",\"n24\"),(\"n7\",\"n49\"),(\"n10\",\"n23\"),(\"n13\",\"n45\"),\n    (\"n18\",\"n46\"),(\"n20\",\"n24\"),(\"n26\",\"n22\"),(\"n29\",\"n21\"),\n    (\"n33\",\"n53\"),(\"n39\",\"n52\"),(\"n48\",\"n16\"),(\"n51\",\"n15\"),\n    (\"n55\",\"n17\"),(\"n57\",\"n19\"),\n]\nSPECIAL = (\"n14\",\"n56\")\n\ndef build_edges(nodes):\n    # group by line keeping order\n    by_line = {}\n    for n in nodes:\n        by_line.setdefault(n[\"line\"], []).append(n)\n    for k in by_line:\n        by_line[k].sort(key=lambda x: x[\"pos\"])\n\n    edges = []\n    eid = 1\n\n    def add(src, dst, typ, pol, dash, speed, weight):\n        nonlocal eid\n        edges.append({\n            \"id\": f\"e{eid}\",\n            \"source\": src, \"target\": dst,\n            \"type\": typ, \"polarity\": pol,\n            \"flow\": {\"dash\": list(dash), \"speed\": float(speed), \"weight\": float(weight)}\n        })\n        eid += 1\n\n    # within-line: positive L→R and negative R→L\n    for line, arr in sorted(by_line.items()):\n        for i in range(len(arr)-1):\n            add(arr[i][\"id\"], arr[i+1][\"id\"], \"within_line\", +1, (6,6), 1.0, 1.8)\n        for i in range(len(arr)-1, 0, -1):\n            add(arr[i][\"id\"], arr[i-1][\"id\"], \"within_line\", -1, (6,6), 1.0, 1.8)\n\n    # cross-sutra: both polarities\n    for s,t in CROSS:\n        add(s,t,\"cross_sutra\", +1, (2,8), 1.3, 1.5)\n        add(t,s,\"cross_sutra\", -1, (2,8), 1.3, 1.5)\n\n    # special ha↔ha: both polarities, thicker\n    add(SPECIAL[0], SPECIAL[1], \"special\", +1, (10,6), 1.6, 3.0)\n    add(SPECIAL[1], SPECIAL[0], \"special\", -1, (10,6), 1.6, 3.0)\n\n    return edges, by_line\n\n# --- 3) Layout (pixel-perfect) ---\nCANVAS = {\"width\": 1800, \"height\": 1400, \"node_w\": 70, \"node_h\": 34}\nMARGIN_X, MARGIN_Y, ROW_GAP, COL_GAP = 40, 40, 85, 110\n\ndef build_layout(by_line):\n    coords = {}\n    for line, arr in sorted(by_line.items()):\n        total_w = len(arr)*CANVAS[\"node_w\"] + (len(arr)-1)*COL_GAP\n        start_x = (CANVAS[\"width\"] - total_w)//2\n        y = MARGIN_Y + (line-1)*ROW_GAP\n        for i, node in enumerate(arr):\n            x = start_x + i*(CANVAS[\"node_w\"] + COL_GAP)\n            coords[node[\"id\"]] = [x, y]\n    return {\"canvas\": CANVAS, \"coords\": coords}\n\n# --- 4) PTK JSON ---\ndef write_ptk(nodes, edges, by_line, path=\"ptk.v1.json\"):\n    ptk = {\n        \"ptk_version\": \"1.0\",\n        \"universe\": \"maheshwara-sutras\",\n        \"meta\": {\n            \"encoding\": \"unicode\",\n            \"transliteration\": \"IAST\",\n            \"created\": datetime.date.today().isoformat()\n        },\n        \"nodes\": nodes,\n        \"edges\": edges,\n        \"groups\": [\n            {\"name\":\"line\",\"key\": line, \"node_ids\":[n[\"id\"] for n in arr]}\n            for line, arr in sorted(by_line.items())\n        ],\n        \"render_hints\": {\n            \"layout\": \"rows_by_line\",\n            \"edge_styles\": {\n                \"within_line\": {\"dash\":[6,6]},\n                \"cross_sutra\": {\"dash\":[2,8]},\n                \"special\": {\"dash\":[10,6], \"weight\": 3}\n            }\n        }\n    }\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(ptk, f, ensure_ascii=False, indent=2)\n\n# --- 5) Layout JSON ---\ndef write_layout(layout, path=\"ptk.layout.json\"):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(layout, f, ensure_ascii=False, indent=2)\n\n# --- 6) Python validator (Pydantic) ---\nPYD = r'''\nfrom __future__ import annotations\nfrom typing import List, Literal, Optional\nfrom pydantic import BaseModel, Field, validator\nimport json, sys, hashlib\n\nEdgeType = Literal[\"within_line\", \"cross_sutra\", \"special\"]\n\nclass Flow(BaseModel):\n    dash: List[int] = Field(..., min_items=2, max_items=2)\n    speed: float = 1.0\n    weight: float = 1.0\n\nclass Node(BaseModel):\n    id: str\n    label: str\n    sanskrit: Optional[str] = None\n    line: int\n    pos: int\n    features: List[str] = []\n\nclass Edge(BaseModel):\n    id: str\n    source: str\n    target: str\n    type: EdgeType\n    polarity: int = Field(..., description=\"+1 or -1\")\n    flow: Flow\n\n    @validator(\"polarity\")\n    def pol_ok(cls, v):\n        if v not in (-1, 1):\n            raise ValueError(\"polarity must be +1 or -1\")\n        return v\n\nclass Group(BaseModel):\n    name: str\n    key: str | int\n    node_ids: List[str]\n\nclass PTK(BaseModel):\n    ptk_version: str\n    universe: str\n    meta: dict\n    nodes: List[Node]\n    edges: List[Edge]\n    groups: List[Group] = []\n    render_hints: dict = {}\n\n    @validator(\"nodes\")\n    def unique_node_ids(cls, nodes):\n        ids = [n.id for n in nodes]\n        if len(ids) != len(set(ids)):\n            raise ValueError(\"duplicate node ids\")\n        return nodes\n\n    @validator(\"edges\")\n    def edge_refs_exist(cls, edges, values):\n        node_ids = {n.id for n in values.get(\"nodes\", [])}\n        for e in edges:\n            if e.source not in node_ids or e.target not in node_ids:\n                raise ValueError(f\"edge {e.id} has missing node ref\")\n        return edges\n\n    @validator(\"edges\")\n    def polarity_pairs(cls, edges):\n        cross = {}\n        for e in edges:\n            if e.type in (\"cross_sutra\",\"special\"):\n                key = tuple(sorted((e.source, e.target))) + (e.type,)\n                cross.setdefault(key, set()).add(e.polarity)\n        for k, pols in cross.items():\n            if pols != {-1, +1}:\n                raise ValueError(f\"edge pair missing opposite polarity for {k}\")\n        return edges\n\ndef sha_file(path: str) -> str:\n    h = hashlib.sha256()\n    with open(path,'rb') as f:\n        for chunk in iter(lambda: f.read(8192), b\"\"):\n            h.update(chunk)\n    return h.hexdigest()\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python ptk_pydantic_validator.py ptk.v1.json\")\n        sys.exit(2)\n    path = sys.argv[1]\n    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n    PTK(**data)  # will raise on invalid\n    print(\"PTK OK:\", path)\n    print(\"sha256:\", sha_file(path))\n'''\ndef write_pyd(path=\"ptk_pydantic_validator.py\"):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(PYD.strip() + \"\\n\")\n\n# --- 7) Rust serde structs ---\nRS_STRUCTS = r'''\nuse serde::{Deserialize, Serialize};\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct Flow {\n    pub dash: (u32, u32),\n    pub speed: f64,\n    pub weight: f64,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\n#[serde(rename_all = \"snake_case\")]\npub enum EdgeType {\n    WithinLine,\n    CrossSutra,\n    Special,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct Node {\n    pub id: String,\n    pub label: String,\n    pub sanskrit: Option<String>,\n    pub line: u32,\n    pub pos: u32,\n    pub features: Vec<String>,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct Edge {\n    pub id: String,\n    pub source: String,\n    pub target: String,\n    #[serde(rename = \"type\")]\n    pub edge_type: EdgeType,\n    pub polarity: i32,\n    pub flow: Flow,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct Group {\n    pub name: String,\n    pub key: serde_json::Value,\n    pub node_ids: Vec<String>,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct PTK {\n    pub ptk_version: String,\n    pub universe: String,\n    pub meta: serde_json::Value,\n    pub nodes: Vec<Node>,\n    pub edges: Vec<Edge>,\n    pub groups: Vec<Group>,\n    pub render_hints: serde_json::Value,\n}\n'''\ndef write_rs_structs(path=\"ptk_rust.rs\"):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(RS_STRUCTS.strip() + \"\\n\")\n\n# --- 8) Rust tests for CI ---\nRS_TESTS = r'''\nuse std::fs;\nuse serde_json::from_str as from_json;\nuse std::collections::{HashMap, HashSet};\n\n// Adjust imports to your crate path if needed:\n// use crate::*; \n// or if placed in a separate crate:\n// use param_tatva_kernel::*;\n\nuse super::{PTK, EdgeType};\n\n#[test]\nfn validate_ptk_kernel() {\n    let data = fs::read_to_string(\"kernel/ptk.v1.json\").expect(\"read ptk.v1.json\");\n    let ptk: PTK = from_json(&data).expect(\"valid PTK\");\n\n    // Unique node ids\n    let mut seen: HashSet<&str> = HashSet::new();\n    for n in &ptk.nodes {\n        assert!(seen.insert(&n.id), \"duplicate node id {}\", n.id);\n    }\n\n    // Edge references exist and polarity is ±1\n    let node_ids: HashSet<_> = ptk.nodes.iter().map(|n| n.id.as_str()).collect();\n    for e in &ptk.edges {\n        assert!(node_ids.contains(e.source.as_str()), \"missing source {}\", e.source);\n        assert!(node_ids.contains(e.target.as_str()), \"missing target {}\", e.target);\n        assert!(e.polarity == 1 || e.polarity == -1, \"polarity must be ±1\");\n    }\n\n    // Cross/special edges must exist in both polarities\n    #[derive(PartialEq, Eq, Hash)]\n    struct Key(String,String,String);\n    let mut pairs: HashMap<Key, HashSet<i32>> = HashMap::new();\n    for e in &ptk.edges {\n        match e.edge_type {\n            EdgeType::CrossSutra | EdgeType::Special => {\n                let (a,b) = if e.source <= e.target { (e.source.clone(), e.target.clone()) } else { (e.target.clone(), e.source.clone()) };\n                let t = match e.edge_type {\n                    EdgeType::CrossSutra => \"cross_sutra\".to_string(),\n                    EdgeType::Special => \"special\".to_string(),\n                    _ => unreachable!()\n                };\n                let key = Key(a,b,t);\n                pairs.entry(key).or_default().insert(e.polarity);\n            }\n            _ => {}\n        }\n    }\n    for (_k, pols) in pairs {\n        assert!(pols.contains(&1) && pols.contains(&-1), \"missing polarity pair\");\n    }\n}\n'''\ndef write_rs_tests(path=\"ptk_rust_tests.rs\"):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(RS_TESTS.strip() + \"\\n\")\n\nif __name__ == \"__main__\":\n    nodes = build_nodes()\n    edges, by_line = build_edges(nodes)\n    layout = build_layout(by_line)\n\n    write_ptk(nodes, edges, by_line, \"ptk.v1.json\")\n    write_layout(layout, \"ptk.layout.json\")\n    write_pyd(\"ptk_pydantic_validator.py\")\n    write_rs_structs(\"ptk_rust.rs\")\n    write_rs_tests(\"ptk_rust_tests.rs\")\n\n    print(\"Generated: ptk.v1.json, ptk.layout.json, ptk_pydantic_validator.py, ptk_rust.rs, ptk_rust_tests.rs\")\n",
      "sha256": "204bdf8a20c3d1f3a80004c0e351485ad0380202074e7bb0602c9aa12bc9ff7c"
    },
    {
      "path": "pt-sim/pt_sim/__init__.py",
      "size": 445,
      "content": "__version__ = \"0.2.0\"\n\n# Explicit re-exports so Ruff treats them as used\nfrom . import physics as physics  # re-export\nfrom . import forces as forces    # re-export\nfrom . import io as io            # re-export\nfrom . import ptk_kernel as ptk_kernel\nfrom .ptk_kernel import PTKKernel, Sound, EmissionConfig\n\n\n\n__all__ = [\"physics\", \"forces\", \"io\", \"__version__\"]\n__all__.append(\"ptk_kernel\")\n__all__ += [\"PTKKernel\", \"Sound\", \"EmissionConfig\"]\n\n",
      "sha256": "205ca734e185cbf31d54a07f6e023b0fefe1facd5477bab4bc49f85d3f468abc"
    },
    {
      "path": "pt-sim/pt_sim/demo_forces.py",
      "size": 3053,
      "content": "from pathlib import Path\nfrom typing import Union\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom .forces.gauge import Particle\nfrom .forces.em import integrate_motion, UniformField\nfrom .forces.potentials import (\n    coulomb_potential,\n    yukawa_potential,\n    string_potential,\n)\nfrom .forces.qcd_toy import flux_tube_map\n\nPathish = Union[str, Path]\n\n\ndef demo_lorentz(outdir: Pathish) -> str:\n    out = Path(outdir)\n    out.mkdir(parents=True, exist_ok=True)\n\n    e = Particle(q=-1.0, m=0.511)\n    field = UniformField(E_vec=(0.0, 0.0, 0.0), B_vec=(0.0, 0.0, 1.0))\n    xs, vs = integrate_motion(e, field, (0.0, 0.0, 0.0), (0.7, 0.0, 0.0), dt=0.05, steps=1200)\n\n    plt.figure()\n    plt.plot(xs[:, 0], xs[:, 1], lw=1.2)\n    plt.axis(\"equal\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Electron in Uniform B (toy NR integrator)\")\n\n    png_path = out / \"force_lorentz_spiral.png\"\n    plt.savefig(png_path, dpi=160, bbox_inches=\"tight\")\n    plt.close()\n    return str(png_path)\n\n\ndef demo_potentials(outdir: Pathish) -> str:\n    out = Path(outdir)\n    out.mkdir(parents=True, exist_ok=True)\n\n    r = np.linspace(0.05, 10.0, 1000)\n    Vc = coulomb_potential(r, 1.0)\n    Vy = yukawa_potential(r, 1.0, 0.7)\n    Vs = string_potential(r, 0.2, 0.0)\n\n    plt.figure()\n    plt.plot(r, Vc, label=\"Coulomb 1/r\")\n    plt.plot(r, Vy, label=\"Yukawa e^{-mr}/r\")\n    plt.plot(r, Vs, label=\"String κr\")\n    plt.ylim(0, 10)\n    plt.legend()\n    plt.xlabel(\"r\")\n    plt.ylabel(\"V(r)\")\n    plt.title(\"Force Potentials (toy)\")\n\n    png_path = out / \"force_potentials.png\"\n    plt.savefig(png_path, dpi=160, bbox_inches=\"tight\")\n    plt.close()\n    return str(png_path)\n\n\ndef demo_flux_tube(outdir: Pathish) -> str:\n    out = Path(outdir)\n    out.mkdir(parents=True, exist_ok=True)\n\n    X, Y, E = flux_tube_map(n=160, sep=10.0, width=1.5)\n\n    plt.figure()\n    \"\"\" plt.imshow(\n        E,\n        origin=\"lower\",\n        extent=[X.min(), X.max(), Y.min(), Y.max()],\n        interpolation=\"bilinear\",\n    ) \"\"\"\n    extent: tuple[float, float, float, float] = (\n        float(X.min()), float(X.max()), float(Y.min()), float(Y.max())\n    )\n    plt.imshow(\n        E,\n        origin=\"lower\",\n        extent=extent,\n        interpolation=\"bilinear\",\n    )\n    plt.colorbar(label=\"Normalized energy density\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Toy Color Flux Tube (QCD-like)\")\n\n    png_path = out / \"force_flux_tube.png\"\n    plt.savefig(png_path, dpi=160, bbox_inches=\"tight\")\n    plt.close()\n    return str(png_path)\n\n\ndef run_all(outdir: Pathish) -> str:\n    out = Path(outdir)\n    out.mkdir(parents=True, exist_ok=True)\n\n    results = {\n        \"spiral_png\": demo_lorentz(out),\n        \"potentials_png\": demo_potentials(out),\n        \"flux_tube_png\": demo_flux_tube(out),\n    }\n\n    with open(out / \"demo_forces_summary.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    return str(out)\n\n\nif __name__ == \"__main__\":\n    import sys\n\n    target = sys.argv[1] if len(sys.argv) > 1 else \"../out\"\n    run_all(target)\n",
      "sha256": "fe8a465d8ee217bf30bfab3be11ff0b794b0295c61245daa9bc0fe0f795184b0"
    },
    {
      "path": "pt-sim/pt_sim/demo_run.py",
      "size": 4098,
      "content": "import json\nfrom pathlib import Path\nimport numpy as np\nfrom typing import Any, Mapping, Sequence, TypeVar, cast\nimport matplotlib.pyplot as plt\n\nfrom .physics.simplified import (\n    longitudinal_profile as simp_longitudinal_profile,\n    lateral_template as simp_lateral_template,\n)\nfrom .physics.accurate import (\n    longitudinal_em as acc_longitudinal_em,\n    lateral_em as acc_lateral_em,\n    digitize_energy as acc_digitize_energy,\n)\nfrom .paramsutra import load\n\nT = TypeVar(\"T\")\n\ndef _get(cfg: Mapping[str, Any], path: Sequence[str], default: T) -> T:\n    cur: Any = cfg\n    for k in path:\n        # Guard: if structure diverges or key missing, return default\n        if not isinstance(cur, Mapping) or k not in cur:\n            return default\n        cur = cur[k]\n    # We’ve walked a Mapping; cast to requested type variable for callers\n    return cast(T, cur)\n\n\ndef run(config_path: str, outdir: str) -> str:\n    \n    cfg = load(config_path)  # TypedDict[Config], still a Mapping for _get\n    out = Path(outdir)\n    out.mkdir(parents=True, exist_ok=True)\n\n    mode = str(_get(cfg, [\"mode\"], \"simplified\"))\n    E = 10.0\n\n    n = int(_get(cfg, [\"ecal_geom\", \"n_cells\"], 32))\n    depth_X0 = float(_get(cfg, [\"shower\", \"depth_X0\"], 20.0))\n    depth = np.linspace(0.0, depth_X0, n)\n\n    if mode == \"simplified\":\n        alpha = float(_get(cfg, [\"shower\", \"alpha\"], 4.0))\n        beta = float(_get(cfg, [\"shower\", \"beta\"], 0.3))\n        sigma_cm = float(_get(cfg, [\"shower\", \"lateral_sigma_cm\"], 1.2))\n        cell_cm = float(_get(cfg, [\"ecal_geom\", \"cell_size_cm\"], 1.0))\n        samp = float(_get(cfg, [\"ecal_geom\", \"sampling_fraction\"], 0.15))\n        noise = float(_get(cfg, [\"ecal_geom\", \"noise_sigma\"], 0.01))\n\n        longE = simp_longitudinal_profile(depth, alpha, beta, E)\n        lat = simp_lateral_template(n, sigma_cm, cell_cm)\n\n        e_true = np.zeros((n, n))\n        for e in longE:\n            e_true += e * lat\n\n        meas = np.clip(\n            e_true * samp + np.random.normal(0.0, noise, size=(n, n)),\n            0.0,\n            None,\n        )\n        tag = \"simplified\"\n\n    else:\n        X0_cm = float(_get(cfg, [\"material\", \"X0_cm\"], 1.0))\n        Ec_MeV = float(_get(cfg, [\"material\", \"Ec_MeV\"], 10.0))\n        RM_cm = float(_get(cfg, [\"material\", \"RM_cm\"], 2.0))\n        samp = float(_get(cfg, [\"material\", \"sampling_fraction\"], 0.15))\n        ly = float(_get(cfg, [\"material\", \"light_yield_pe_per_GeV\"], 100.0))\n        enoise = float(_get(cfg, [\"material\", \"electronics_noise_sigma\"], 0.01))\n        cell_cm = float(_get(cfg, [\"ecal_geom\", \"cell_size_cm\"], 1.0))\n\n        longE = acc_longitudinal_em(depth, E, X0_cm=X0_cm, Ec_MeV=Ec_MeV)\n        lat = acc_lateral_em(n, cell_cm, RM_cm)\n\n        e_true = np.zeros((n, n))\n        for e in longE:\n            e_true += e * lat\n\n        rng = np.random.default_rng(12345)\n        meas = acc_digitize_energy(\n            e_true,\n            samp,\n            ly,\n            enoise,\n            rng=rng,\n        )\n        tag = \"accurate\"\n\n    # Plots\n    plt.figure()\n    plt.imshow(meas, origin=\"lower\", interpolation=\"nearest\")\n    plt.colorbar(label=\"Energy (GeV)\")\n    plt.title(f\"ECAL Energy Image (electron, {tag})\")\n    plt.savefig(out / f\"ecal_image_{tag}.png\", dpi=160, bbox_inches=\"tight\")\n    plt.close()\n\n    plt.figure()\n    plt.plot(depth, longE)\n    plt.xlabel(\"Depth (X0 units)\")\n    plt.ylabel(\"Energy per layer (GeV)\")\n    plt.title(f\"Longitudinal Profile (electron, {tag})\")\n    plt.savefig(out / f\"longitudinal_profile_{tag}.png\", dpi=160, bbox_inches=\"tight\")\n    plt.close()\n\n    with open(out / f\"summary_{tag}.json\", \"w\") as f:\n        json.dump(\n            {\n                \"mode\": mode,\n                \"electron_energy_GeV\": E,\n                \"sum_measured_GeV\": float(meas.sum()),\n                \"n_cells\": int(n * n),\n                \"config_used\": str(Path(config_path).resolve()),\n            },\n            f,\n            indent=2,\n        )\n\n    return str(out)\n\n\nif __name__ == \"__main__\":\n    import sys\n\n    run(sys.argv[1], sys.argv[2] if len(sys.argv) > 2 else \"../out\")\n",
      "sha256": "ac093c8ac49c861cd5c21fde09b36114caf2c39db71f4fd8007affa5b0b11143"
    },
    {
      "path": "pt-sim/pt_sim/detector/bridge.py",
      "size": 1487,
      "content": "# ruff: noqa: E501\n# ruff: noqa: E701\nimport numpy as np\nfrom typing import Dict, Any, Tuple\n\n\ndef _node_to_xy(node_line: int, node_pos: int, n: int) -> Tuple[int, int]:\n    # Simple deterministic mapping onto a grid: ring-by-line, angle-by-pos\n    # Tweak later if you have a real geometry.\n    r = int((n*0.15) + (node_line/14.0)*(n*0.35))\n    c = int((n*0.15) + (node_pos/9.0)*(n*0.6))\n    r = max(0, min(n-1, r))\n    c = max(0, min(n-1, c))\n    return r, c\n\n\n\ndef kernel_to_ecal_image(emission: Dict[str, Any], n: int = 32, e_scale: float = 1.0, **kwargs) -> np.ndarray:\n    # Back-compat alias\n    if \"E_scale\" in kwargs and kwargs[\"E_scale\"] is not None:\n        e_scale = kwargs[\"E_scale\"]\n        \n    \"\"\"Convert kernel outputs to a toy ECAL-like 2D image.\"\"\"\n    img = np.zeros((n, n), float)\n\n    # Particles -> localized Gaussian deposits\n    for p in emission.get(\"particles\", []):\n        line = int(p[\"Q\"].get(\"line\", 1))\n        pos = int(p[\"Q\"].get(\"pos\", 1))\n        r, c = _node_to_xy(line, pos, n)\n        rr, cc = np.meshgrid(np.arange(n), np.arange(n), indexing=\"ij\")\n        sigma = max(1.0, 0.6 + 0.04*p[\"energy\"])\n        blob = np.exp(-0.5*((rr-r)**2+(cc-c)**2)/(sigma**2))\n        img += e_scale * p[\"energy\"] * blob\n\n    # Fields -> diffuse background (proportional to energy and edge count)\n    total_field_E = sum(f[\"energy\"] for f in emission.get(\"fields\", []))\n    if total_field_E > 0:\n        img += e_scale * 0.3 * total_field_E / (n*n)\n\n    return img\n",
      "sha256": "ca18df013f13d3db686e13a81826835167f58fa91052c889438cdc8ecc253b6d"
    },
    {
      "path": "pt-sim/pt_sim/detector/run_from_kernel.py",
      "size": 481,
      "content": "import os\nimport json\nimport numpy as np\nfrom .bridge import kernel_to_ecal_image\n\ndef run(emission_json=\"out/ptk_emission_result.json\", out_npz=\"out/detected_event.npz\", n=32):\n    os.makedirs(os.path.dirname(out_npz), exist_ok=True)\n    res = json.load(open(emission_json, \"r\", encoding=\"utf-8\"))\n    img = kernel_to_ecal_image(res, n=n, e_scale=1.0)\n    np.savez_compressed(out_npz, ecal=img.astype(np.float32))\n    print(\"Wrote\", out_npz)\n\nif __name__ == \"__main__\":\n    run()\n",
      "sha256": "2fc1309eac1f93be427a95fcb30f19f59236824fe97c54967a3b11562d863f9e"
    },
    {
      "path": "pt-sim/pt_sim/fastcore.py",
      "size": 719,
      "content": "\"\"\"\nFast-path wrappers. Prefer Rust (pt_core_py) if available, else Python fallback.\n\"\"\"\n\ntry:\n    import pt_core_py as _rc\n    HAVE_RUST = True\nexcept Exception:\n    _rc = None\n    HAVE_RUST = False\n\n\ndef kk_winding_mass2(m1, m2, w1, w2, r1, r2, alpha_prime):\n    if HAVE_RUST:\n        return float(\n            _rc.kk_winding_mass2_py(\n                int(m1),\n                int(m2),\n                int(w1),\n                int(w2),\n                float(r1),\n                float(r2),\n                float(alpha_prime),\n            )\n        )\n    # Python fallback\n    return (\n        (m1 / r1) ** 2\n        + (m2 / r2) ** 2\n        + (w1 * r1 / alpha_prime) ** 2\n        + (w2 * r2 / alpha_prime) ** 2\n    )\n",
      "sha256": "18ed01e1c5492463f66e0d8f36990762ea5517be17a1c392646614d546bb51e1"
    },
    {
      "path": "pt-sim/pt_sim/forces/analytic.py",
      "size": 259,
      "content": "def helix_radius(v_perp, q_over_m, B_mag):\n    \"\"\"r = v_perp / omega,  omega = |q|B/m = |q_over_m|*B.\"\"\"\n    omega = abs(q_over_m) * abs(B_mag)\n    return abs(v_perp) / max(omega, 1e-16)\n\n\ndef cyclotron_frequency(q_over_m, B_mag):\n    return q_over_m * B_mag\n",
      "sha256": "fcbe659881c7079ca99f2bd8694418727426f4b86bcbc26adfe1f2273795eb68"
    },
    {
      "path": "pt-sim/pt_sim/forces/boris.py",
      "size": 1799,
      "content": "import numpy as np\nfrom .gauge import Particle  # noqa: F401  (used in type hints)\n\n\ndef boris_step(q_over_m, dt, v, E, B, relativistic=False, c=1.0):\n    \"\"\"\n    Single Boris push.\n    If relativistic=True, uses gamma from v at the beginning of the step.\n    Units: c can be 1.0 (natural units). Keep speeds well below c if non-relativistic.\n    \"\"\"\n    v = np.asarray(v, float)\n    E = np.asarray(E, float)\n    B = np.asarray(B, float)\n\n    if relativistic:\n        v2 = float(np.dot(v, v))\n        gamma = 1.0 / np.sqrt(max(1.0 - v2 / (c * c), 1e-12))\n    else:\n        gamma = 1.0\n\n    # Half electric kick\n    v_minus = v + (q_over_m * E) * (dt * 0.5) / gamma\n\n    # Magnetic rotation\n    t = q_over_m * B * (dt * 0.5) / gamma\n    t2 = float(np.dot(t, t))\n    v_prime = v_minus + np.cross(v_minus, t)\n    s = 2.0 * t / (1.0 + t2)\n    v_plus = v_minus + np.cross(v_prime, s)\n\n    # Half electric kick\n    v_new = v_plus + (q_over_m * E) * (dt * 0.5) / gamma\n    return v_new\n\n\ndef integrate_motion_boris(\n    p: Particle,\n    E_func,\n    B_func,\n    x0,\n    v0,\n    dt,\n    steps,\n    relativistic=False,\n    c=1.0,\n):\n    \"\"\"\n    Integrate trajectory with Boris pusher. E_func(t, x)->E, B_func(t, x)->B.\n    Returns arrays xs, vs (shape [steps, 3]).\n    \"\"\"\n    x = np.array(x0, float)\n    v = np.array(v0, float)\n    xs = np.zeros((steps, 3))\n    vs = np.zeros((steps, 3))\n\n    q_over_m = p.q / max(p.m, 1e-12)\n    t = 0.0\n\n    for i in range(steps):\n        E = np.asarray(E_func(t, x), float)\n        B = np.asarray(B_func(t, x), float)\n\n        # Boris velocity update\n        v = boris_step(q_over_m, dt, v, E, B, relativistic=relativistic, c=c)\n\n        # Position update (leapfrog consistent)\n        x = x + v * dt\n        xs[i] = x\n        vs[i] = v\n        t += dt\n\n    return xs, vs\n",
      "sha256": "367cd1587878f55017cfca7ad756f609b7cd55604763fef7bfbf307e79f7a1ea"
    },
    {
      "path": "pt-sim/pt_sim/forces/em.py",
      "size": 966,
      "content": "import numpy as np\nfrom .gauge import Field, lorentz_force\n\n\nclass UniformField(Field):\n    def __init__(self, E_vec=(0, 0, 0), B_vec=(0, 0, 1)):\n        self._E = np.array(E_vec, float)\n        self._B = np.array(B_vec, float)\n\n    def E(self, t, x):\n        return self._E\n\n    def B(self, t, x):\n        return self._B\n\n\ndef integrate_motion(particle, field, x0, v0, dt, steps):\n    x = np.array(x0, float)\n    v = np.array(v0, float)\n    xs = np.zeros((steps, 3))\n    vs = np.zeros((steps, 3))\n\n    for i in range(steps):\n        E = field.E(i * dt, x)\n        B = field.B(i * dt, x)\n        a = lorentz_force(particle, v, E, B) / max(particle.m, 1e-9)\n\n        v_half = v + 0.5 * dt * a\n        x = x + dt * v_half\n\n        E2 = field.E((i + 1) * dt, x)\n        B2 = field.B((i + 1) * dt, x)\n        a2 = lorentz_force(particle, v_half, E2, B2) / max(particle.m, 1e-9)\n\n        v = v_half + 0.5 * dt * a2\n\n        xs[i] = x\n        vs[i] = v\n\n    return xs, vs\n",
      "sha256": "d95a4fd9b6063a8271618a231da387ab9923be77b97f461eb575625309c71268"
    },
    {
      "path": "pt-sim/pt_sim/forces/gauge.py",
      "size": 406,
      "content": "from dataclasses import dataclass\nimport numpy as np\n\n\n@dataclass\nclass Particle:\n    q: float\n    m: float\n\n\nclass Field:\n    def E(self, t, x):\n        return np.zeros(3)\n\n    def B(self, t, x):\n        return np.zeros(3)\n\n\ndef lorentz_force(particle: Particle, v, E, B):\n    v = np.asarray(v, float)\n    E = np.asarray(E, float)\n    B = np.asarray(B, float)\n    return particle.q * (E + np.cross(v, B))\n",
      "sha256": "94506950e9edcefa723613608251b77060b92d899ce20d7f9e897bd824d154ea"
    },
    {
      "path": "pt-sim/pt_sim/forces/potentials.py",
      "size": 330,
      "content": "import numpy as np\n\n\ndef coulomb_potential(r, alpha: float = 1.0):\n    r = np.maximum(r, 1e-6)\n    return alpha / r\n\n\ndef yukawa_potential(r, g: float = 1.0, m: float = 1.0):\n    r = np.maximum(r, 1e-6)\n    return (g**2) * np.exp(-m * r) / r\n\n\ndef string_potential(r, kappa: float = 1.0, c: float = 0.0):\n    return kappa * r + c\n",
      "sha256": "0c92d05cefa9a23a1d80bcc34806aeab7c475a57e17aa6f17f673345ede13655"
    },
    {
      "path": "pt-sim/pt_sim/forces/qcd_toy.py",
      "size": 492,
      "content": "import numpy as np\n\n\ndef flux_tube_map(n: int = 160, sep: float = 10.0, width: float = 1.5):\n    x = np.linspace(-sep, sep, n)\n    y = np.linspace(-sep, sep, n)\n    X, Y = np.meshgrid(x, y, indexing=\"xy\")\n\n    tube = np.exp(-0.5 * (Y / width) ** 2) * np.exp(-0.1 * ((np.abs(X) - sep / 2) ** 2))\n    blob1 = np.exp(-0.5 * (((X + sep / 2) ** 2 + Y**2) / (width**2)))\n    blob2 = np.exp(-0.5 * (((X - sep / 2) ** 2 + Y**2) / (width**2)))\n\n    E = tube + 0.4 * (blob1 + blob2)\n    return X, Y, E\n",
      "sha256": "20cae966386ad605d05bb208af848b78ba9be0b23e17124b2f445ae04ed951e6"
    },
    {
      "path": "pt-sim/pt_sim/io/hepmc.py",
      "size": 543,
      "content": "from pathlib import Path\n\n\ndef write_ascii_minimal(path, particles):\n    \"\"\"\n    Very small, HepMC-like ASCII for tests. 'particles' is list of dicts with keys:\n      id, px, py, pz, E, status\n    \"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        f.write(\"# Minimal HepMC-like ASCII (ParamTatva)\\n\")\n        for p in particles:\n            f.write(\n                f\"P {p['id']} {p['px']} {p['py']} {p['pz']} {p['E']} {p.get('status', 1)}\\n\"\n            )\n    return str(path)\n",
      "sha256": "a72ac60d8778f781433515a3c02804682a16d234b7a9114d09be8e548a21badd"
    },
    {
      "path": "pt-sim/pt_sim/io/rootio.py",
      "size": 558,
      "content": "from pathlib import Path\nimport numpy as np\n\n\ndef write_event_npz(path, arrays: dict):\n    \"\"\"Safe default: writes numpy .npz file so tests pass without ROOT.\"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    np.savez_compressed(path, **arrays)\n    return str(path)\n\n\ndef available():\n    \"\"\"Return a tiny capability struct; extend when ROOT/uproot present.\"\"\"\n    try:\n        import uproot  # noqa: F401\n        return {\"root\": True, \"backend\": \"uproot\"}\n    except Exception:\n        return {\"root\": False, \"backend\": \"npz\"}\n",
      "sha256": "fd4d29c229da84a938ec505e317cdd82135bed10e1506d2d3a6e483268e9af42"
    },
    {
      "path": "pt-sim/pt_sim/paramsutra.py",
      "size": 863,
      "content": "from typing import TypedDict, cast\nimport yaml\n\n\nclass ShowerCfg(TypedDict, total=False):\n    alpha: float\n    beta: float\n    depth_X0: float\n    lateral_sigma_cm: float\n\n\nclass EcalGeomCfg(TypedDict, total=False):\n    n_cells: int\n    cell_size_cm: float\n    sampling_fraction: float\n    noise_sigma: float\n\n\nclass MaterialCfg(TypedDict, total=False):\n    X0_cm: float\n    Ec_MeV: float\n    RM_cm: float\n    sampling_fraction: float\n    light_yield_pe_per_GeV: float\n    electronics_noise_sigma: float\n\n\nclass Config(TypedDict, total=False):\n    mode: str\n    shower: ShowerCfg\n    ecal_geom: EcalGeomCfg\n    material: MaterialCfg\n\n\ndef load(path: str) -> Config:\n    with open(path, \"r\") as f:\n        data = yaml.safe_load(f)\n    # We accept partial configs; callers should guard with defaults.\n    return cast(Config, data if isinstance(data, dict) else {})\n",
      "sha256": "b5553cc475ff0141971d4711968bb645467e10b51552218adefeb556189992a5"
    },
    {
      "path": "pt-sim/pt_sim/physics/accurate.py",
      "size": 1862,
      "content": "import numpy as np\n\n\ndef lateral_em(n, cell_size_cm, RM_cm, core_frac: float = 0.8):\n    idx = np.arange(n) - (n - 1) / 2\n    X, Y = np.meshgrid(idx, idx, indexing=\"xy\")\n    X = X * cell_size_cm\n    Y = Y * cell_size_cm\n    R2 = X**2 + Y**2\n    sc = 0.3 * RM_cm\n    st = 1.0 * RM_cm\n    Gc = np.exp(-0.5 * R2 / (sc**2))\n    Gt = np.exp(-0.5 * R2 / (st**2))\n    M = core_frac * Gc + (1.0 - core_frac) * Gt\n    return M / M.sum()\n\n\ndef longitudinal_em(depth, E, X0_cm: float, Ec_MeV: float):\n    \"\"\"\n    Toy, PDG-inspired gamma-like longitudinal profile with energy closure.\n    depth: array in X0 units (already scaled)\n    E: GeV\n    X0_cm, Ec_MeV are accepted for future realism; not used in the toy shape.\n    \"\"\"\n    t = np.asarray(depth, float)\n    # Shape parameter grows (weakly) with E/Ec; keep >=1\n    a = float(np.log(max(E * 1e3 / max(Ec_MeV, 1e-6), 1.01)) + 1.0)\n    b = 1.0\n    f = np.power(np.maximum(t, 1e-12), a - 1.0) * np.exp(-b * t)\n    wsum = float(f.sum())\n    if wsum <= 0.0:\n        return np.ones_like(t) * (E / max(t.size, 1))\n    return E * (f / wsum)\n\n\ndef digitize_energy(\n    ecal_true,\n    sampling_fraction,\n    light_yield_pe_per_GeV,\n    electronics_noise_GeV,\n    rng,\n):\n    mean_signal = sampling_fraction * ecal_true\n    mean_pe = mean_signal * light_yield_pe_per_GeV\n    # Poisson at low mean, Gaussian approx at high mean\n    out = np.empty_like(mean_signal, dtype=float)\n    low = mean_pe < 20.0\n    high = ~low\n    out[low] = rng.poisson(mean_pe[low]).astype(float) / np.maximum(\n        light_yield_pe_per_GeV, 1e-12\n    )\n    out[high] = (\n        mean_signal[high]\n        + rng.normal(\n            0.0,\n            np.sqrt(mean_pe[high]) / np.maximum(light_yield_pe_per_GeV, 1e-12),\n        )\n    )\n    out += rng.normal(0.0, electronics_noise_GeV, size=out.shape)\n    out = np.clip(out, 0.0, None)\n    return out\n\n\n \n",
      "sha256": "a72bf70657b95fed9d015f3c8e002d1872c5456c3962c90c538e76a848a680d6"
    },
    {
      "path": "pt-sim/pt_sim/physics/simplified.py",
      "size": 858,
      "content": "import numpy as np\n\n\n\ndef lateral_template(n, sigma_cm, cell_size_cm):\n    idx = np.arange(n) - (n - 1) / 2\n    X, Y = np.meshgrid(idx, idx, indexing=\"xy\")\n    X = X * cell_size_cm\n    Y = Y * cell_size_cm\n    G = np.exp(-0.5 * (X**2 + Y**2) / (sigma_cm**2))\n    return G / G.sum()\n\n\ndef longitudinal_profile(depth, alpha, beta, E):\n    \"\"\"\n    Toy EM longitudinal profile (gamma-like).\n    depth: array in radiation-length units\n    alpha,beta: shape/scale-like toy params\n    Returns per-layer energy allocating exactly sum(...) == E.\n    \"\"\"\n    t = np.asarray(depth, float)\n    # Positive, normalized weights\n    f = np.power(np.maximum(t, 1e-12), alpha - 1.0) * np.exp(-beta * t)\n    wsum = float(f.sum())\n    if wsum <= 0.0:\n        # fallback to uniform if bad params\n        return np.ones_like(t) * (E / max(t.size, 1))\n    return E * (f / wsum)\n\n \n",
      "sha256": "36154f2549cefd64b88b8e10576d909932a68d62ddca7835f94a4a5a057ae388"
    },
    {
      "path": "pt-sim/pt_sim/ptk_kernel.py",
      "size": 12664,
      "content": "# ruff: noqa: E501\n# ruff: noqa: E701\nfrom __future__ import annotations\n\n# Write a robust, reference Python kernel for \"sound emission\" interactions over PTK.\n# It will:\n# - Load PTK v1 JSON\n# - Define Sound packets with polarity (+1/-1), energy, phase, frequency\n# - Propagate packets along edges respecting polarity\n# - Handle interactions (partial/total cancellation) of opposite-polarity packets on same edge\n# - Emit particles and fields from residual energies (can produce multiple outputs)\n# - Return a structured result JSON for study (generation + detection stubs)\n\n\nimport json\nimport math\nimport uuid\nfrom dataclasses import dataclass, asdict\n\nfrom collections import defaultdict\nfrom typing import DefaultDict, Dict, List, Tuple, TypeAlias, Any, Optional, Union\n\n\n\nNodeId: TypeAlias = str          # node ids are \"n1\", \"n4\", ...\nPolarity: TypeAlias = int         # +1 or -1\nEdgeKey: TypeAlias = Tuple[NodeId, Polarity]\nEdge:    TypeAlias = Dict[str, Any]\n\n\nptk_path = \"ptk.v1.json\"\n\n# ---------- Data Models ----------\n\n@dataclass\nclass Sound:\n    node_id: str                # start node\n    polarity: int               # +1 or -1\n    energy: float               # arbitrary units\n    frequency: float            # Hz (relative)\n    phase: float                # radians\n    coherence: float = 1.0      # 0..1 quality factor\n    label: str = \"\"             # optional tag\n\n@dataclass\nclass EmissionConfig:\n    step_len: float = 1.0               # normalized distance per step along an edge\n    max_steps: int = 50                 # hard cap on steps\n    split_decay: float = 0.9            # loss when splitting at nodes\n    line_bias: float = 1.0              # multiplier for within_line edges\n    cross_bias: float = 0.85            # multiplier for cross_sutra edges\n    special_bias: float = 1.25          # multiplier for special edges\n    cancel_bandwidth: float = 0.05      # relative frequency tolerance for cancellation\n    cancel_phase_tol: float = 0.6       # radians tolerance for near-antiphase\n    cancel_efficiency: float = 0.7      # fraction of matched energies that cancel\n    particle_E_thresh: float = 2.0      # minimum localized residual energy to spawn particle\n    field_E_thresh: float = 1.0         # minimum delocalized residual energy to spawn field\n    coherence_thresh: float = 0.5       # minimum coherence to spawn structured outputs\n    max_outputs: int = 64               # safety cap for outputs\n\n@dataclass\nclass Particle:\n    id: str\n    locus: str                 # node id\n    energy: float\n    mass2: float               # toy relation: proportional to energy\n    Q: Dict[str, float]        # charges/labels derived from node lineage\n\n@dataclass\nclass Field:\n    id: str\n    support_edges: List[str]   # edge ids where the field is distributed\n    energy: float\n    strength: float            # aggregate measure\n    mode: str                  # \"vector\" | \"scalar\" (toy)\n\n@dataclass\nclass DetectionEvent:\n    id: str\n    kind: str                  # \"particle\" | \"field\"\n    ref_id: str                # particle.id or field.id\n    confidence: float\n\n# ---------- Kernel Implementation ----------\n\nclass PTKKernel:\n    def __init__(self, ptk: Dict[str, Any]):\n        self.ptk = ptk\n\n        self.nodes: Dict[NodeId, Dict[str, Any]] = {n[\"id\"]: n for n in ptk[\"nodes\"]}\n\n\n        # keyed by (node_id, polarity)\n        self.out_edges: DefaultDict[EdgeKey, List[Edge]] = defaultdict(list)\n        self.in_edges:  DefaultDict[EdgeKey, List[Edge]] = defaultdict(list)\n\n        # build adjacency\n        for e in ptk[\"edges\"]:\n            src: NodeId = e[\"source\"]\n            tgt: NodeId = e[\"target\"]\n            pol: Polarity = int(e[\"polarity\"])\n            self.out_edges.setdefault((src, pol), []).append(e)\n            self.in_edges.setdefault((tgt, pol), []).append(e)\n\n\n\n        self.edge_index: Dict[str, Edge] = {e[\"id\"]: e for e in ptk[\"edges\"]}\n\n        # direct alias; if you prefer, keep a separate dict with the same type\n        self.next_edges: Dict[EdgeKey, List[Edge]] = dict(self.out_edges)\n\n        for (src, pol), arr in self.out_edges.items():\n            self.next_edges[(src, pol)] = arr\n\n    @staticmethod\n    def edge_key(node_id: Union[str, object], polarity: Union[int, float, object]) -> EdgeKey:\n        # coerce node_id to str and polarity to canonical ±1 ints (or just int(polarity) if you prefer)\n        nid = str(node_id)\n        if isinstance(polarity, float):\n            pol_i = 1 if polarity >= 0 else -1\n        elif isinstance(polarity, int):\n            pol_i = 1 if polarity >= 0 else -1\n        else:\n            # fall back if something weird sneaks in\n            pol_i = 1\n        return (nid, pol_i)\n\n    def _edge_gain(self, e):\n        typ = e[\"type\"]\n        if typ == \"within_line\":   bias = 1.0\n        elif typ == \"cross_sutra\": bias = 0.9\n        elif typ == \"special\":     bias = 1.2\n        else:                      bias = 1.0\n        # incorporate flow weight as conductivity\n        return bias * float(e[\"flow\"].get(\"weight\", 1.0))\n\n    def _charges_from_node(self, node_id: str) -> Dict[str, float]:\n        n = self.nodes[node_id]\n        # Simple toy rule-set; customize as needed\n        return {\n            \"line\": float(n[\"line\"]),\n            \"pos\": float(n[\"pos\"]),\n            \"vowel_bias\": 1.0 if any(f in n.get(\"features\", []) for f in [\"vowel\"]) else 0.0\n        }\n\n    def simulate_emission(self, positive: List[Sound], negative: List[Sound], cfg: Optional[EmissionConfig]=None):\n        if cfg is None:\n            cfg = EmissionConfig()\n\n        # Packets: each has (edge_id, progress 0..1, energy, freq, phase, coherence)\n        # Start at node; we place them on a virtual 0-length edge and immediately expand along outgoing edges.\n        packets = []  # (edge_id, progress, energy, freq, phase, coh)\n        def seed_packets(sounds: List[Sound]):\n            for s in sounds:\n                # Outgoing edges of the matching polarity from the node\n                outs = self.next_edges.get((s.node_id, s.polarity), [])\n                if not outs:\n                    continue\n                share = s.energy / len(outs)\n                for e in outs:\n                    packets.append([e[\"id\"], 0.0, share, s.frequency, s.phase, s.coherence, s.polarity])\n\n        seed_packets(positive)\n        seed_packets(negative)\n\n        # For interaction bookkeeping at each time step: map edge_id -> list of packets on it\n        particles: List[Particle] = []\n        fields: List[Field] = []\n        dets: List[DetectionEvent] = []\n\n        def spawn_particle(node_id: str, E: float, coh: float):\n            p = Particle(\n                id=str(uuid.uuid4()),\n                locus=node_id,\n                energy=E,\n                mass2=E * (0.8 + 0.4*coh),  # toy: coherence lifts effective mass2\n                Q=self._charges_from_node(node_id)\n            )\n            particles.append(p)\n            dets.append(DetectionEvent(id=str(uuid.uuid4()), kind=\"particle\", ref_id=p.id, confidence=min(1.0, 0.6+0.4*coh)))\n\n        def spawn_field(edge_ids: List[str], E: float, coh: float):\n            f = Field(\n                id=str(uuid.uuid4()),\n                support_edges=edge_ids[:32],  # cap\n                energy=E,\n                strength=E * (0.5 + 0.5*coh),\n                mode=\"vector\" if coh > 0.7 else \"scalar\"\n            )\n            fields.append(f)\n            dets.append(DetectionEvent(id=str(uuid.uuid4()), kind=\"field\", ref_id=f.id, confidence=min(1.0, 0.5+0.5*coh)))\n\n        step = 0\n        while step < cfg.max_steps and packets and (len(particles)+len(fields) < cfg.max_outputs):\n            step += 1\n\n            # Advance packets and apply per-edge gain/decay\n            by_edge: Dict[str, List[List[float]]] = {}\n            for pk in packets:\n                edge_id, prog, E, fr, ph, coh, pol = pk\n                e = self.edge_index[edge_id]\n                gain = self._edge_gain(e)\n                # advance\n                prog_new = prog + cfg.step_len\n                E_eff = E * gain\n                pk[1] = prog_new\n                pk[2] = E_eff\n                by_edge.setdefault(edge_id, []).append(pk)\n\n            new_packets = []\n\n            # Resolve interactions on each edge\n            for edge_id, plist in by_edge.items():\n                # split by polarity\n                P = [p for p in plist if p[6] == +1]\n                N = [p for p in plist if p[6] == -1]\n\n                # Handle cancellations on overlapping segments\n                # We approximate: if packets exist of both polarities, cancel a fraction of matched energy\n                if P and N:\n                    # match by nearest freq & near-antiphase\n                    P.sort(key=lambda x: x[3])\n                    N.sort(key=lambda x: x[3])\n                    i=j=0\n                    \n                    E_field_accum = 0.0\n                    while i < len(P) and j < len(N):\n                        p, n = P[i], N[j]\n                        df = abs(p[3] - n[3]) / max(1e-6, (p[3] + n[3]) / 2.0)\n                        dphi = abs(((p[4] - n[4] + math.pi) % (2*math.pi)) - math.pi)  # wrap to [0,pi]\n                        if df <= cfg.cancel_bandwidth and dphi >= (math.pi - cfg.cancel_phase_tol):\n                            # amount that cancels is limited by min energy and coherence\n                            k = cfg.cancel_efficiency * min(p[5], n[5])\n                            dE = k * min(p[2], n[2])\n                            p[2] -= dE\n                            n[2] -= dE\n                            E_field_accum += dE * 0.5  # portion radiates as field\n                        # move the pointer with larger frequency to get pairings\n                        if p[3] <= n[3]: i += 1\n                        else: j += 1\n\n                    # spawn field if meaningful delocalized residual from cancellations\n                    if E_field_accum >= cfg.field_E_thresh:\n                        # coherent measure = mean coherence of packets encountered\n                        coh_vals = [p[5] for p in P+N]\n                        coh_mean = sum(coh_vals)/len(coh_vals)\n                        spawn_field([edge_id], E_field_accum, coh_mean)\n\n                # Any packet that reached end of edge spawns into outgoing edges of same polarity from target node\n                e = self.edge_index[edge_id]\n                # target is e['target']\n                for pk in plist:\n                    edge_id, prog, E, fr, ph, coh, pol = pk\n                    if E <= 1e-6:\n                        continue\n                    if prog < 1.0:\n                        # still on the edge\n                        new_packets.append(pk)\n                    else:\n                        # reached end: split to outgoing edges from target\n                        #outs = self.next_edges.get((e[\"target\"], pol), [])\n                        outs = self.next_edges.get(self.edge_key(e[\"target\"], pol), [])\n\n                        if not outs:\n                            # localize energy at node; possible particle\n                            if E >= cfg.particle_E_thresh and coh >= cfg.coherence_thresh:\n                                spawn_particle(e[\"target\"], E, coh)\n                            continue\n                        share = (E * cfg.split_decay) / len(outs)\n                        for oe in outs:\n                            new_packets.append([oe[\"id\"], 0.0, share, fr, ph, coh, pol])\n\n            packets = new_packets\n\n        result = {\n            \"particles\": [asdict(p) for p in particles],\n            \"fields\": [asdict(f) for f in fields],\n            \"detections\": [asdict(d) for d in dets],\n            \"steps\": step\n        }\n        return result\n\n\nif __name__ == \"__main__\":\n    # ---------- Example usage on your current PTK ----------\n    with open(ptk_path, \"r\", encoding=\"utf-8\") as f:\n        ptk = json.load(f)\n\n    kernel = PTKKernel(ptk)\n\n    # Example emission: two lobes (positive starting at n1 and negative starting at n4, same band)\n    positive = [Sound(node_id=\"n1\", polarity=+1, energy=5.0, frequency=440.0, phase=0.0, coherence=0.9, label=\"P1\")]\n    negative = [Sound(node_id=\"n4\", polarity=-1, energy=5.0, frequency=440.0, phase=math.pi, coherence=0.9, label=\"N1\")]\n\n    res = kernel.simulate_emission(positive, negative, EmissionConfig())\n    out_json = \"ptk_emission_result.json\"\n    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n        json.dump(res, f, indent=2)\n\n    print(\"Wrote\", out_json)\n    print(\"Particles:\", len(res[\"particles\"]), \"Fields:\", len(res[\"fields\"]), \"Detections:\", len(res[\"detections\"]))\n",
      "sha256": "0b7ff7a4828ea3ebd4f4c093d1dc87a6ea1e0ab056ce0fcb1396342435853865"
    },
    {
      "path": "ptk_pydantic_validator.py",
      "size": 2563,
      "content": "from __future__ import annotations\nfrom typing import List, Literal, Optional\nfrom pydantic import BaseModel, Field, validator\nimport json, sys, hashlib\n\nEdgeType = Literal[\"within_line\", \"cross_sutra\", \"special\"]\n\nclass Flow(BaseModel):\n    dash: List[int] = Field(..., min_items=2, max_items=2)\n    speed: float = 1.0\n    weight: float = 1.0\n\nclass Node(BaseModel):\n    id: str\n    label: str\n    sanskrit: Optional[str] = None\n    line: int\n    pos: int\n    features: List[str] = []\n\nclass Edge(BaseModel):\n    id: str\n    source: str\n    target: str\n    type: EdgeType\n    polarity: int = Field(..., description=\"+1 or -1\")\n    flow: Flow\n\n    @validator(\"polarity\")\n    def pol_ok(cls, v):\n        if v not in (-1, 1):\n            raise ValueError(\"polarity must be +1 or -1\")\n        return v\n\nclass Group(BaseModel):\n    name: str\n    key: str | int\n    node_ids: List[str]\n\nclass PTK(BaseModel):\n    ptk_version: str\n    universe: str\n    meta: dict\n    nodes: List[Node]\n    edges: List[Edge]\n    groups: List[Group] = []\n    render_hints: dict = {}\n\n    @validator(\"nodes\")\n    def unique_node_ids(cls, nodes):\n        ids = [n.id for n in nodes]\n        if len(ids) != len(set(ids)):\n            raise ValueError(\"duplicate node ids\")\n        return nodes\n\n    @validator(\"edges\")\n    def edge_refs_exist(cls, edges, values):\n        node_ids = {n.id for n in values.get(\"nodes\", [])}\n        for e in edges:\n            if e.source not in node_ids or e.target not in node_ids:\n                raise ValueError(f\"edge {e.id} has missing node ref\")\n        return edges\n\n    @validator(\"edges\")\n    def polarity_pairs(cls, edges):\n        cross = {}\n        for e in edges:\n            if e.type in (\"cross_sutra\",\"special\"):\n                key = tuple(sorted((e.source, e.target))) + (e.type,)\n                cross.setdefault(key, set()).add(e.polarity)\n        for k, pols in cross.items():\n            if pols != {-1, +1}:\n                raise ValueError(f\"edge pair missing opposite polarity for {k}\")\n        return edges\n\ndef sha_file(path: str) -> str:\n    h = hashlib.sha256()\n    with open(path,'rb') as f:\n        for chunk in iter(lambda: f.read(8192), b\"\"):\n            h.update(chunk)\n    return h.hexdigest()\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python ptk_pydantic_validator.py ptk.v1.json\")\n        sys.exit(2)\n    path = sys.argv[1]\n    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n    PTK(**data)  # will raise on invalid\n    print(\"PTK OK:\", path)\n    print(\"sha256:\", sha_file(path))\n",
      "sha256": "e72870b6b8731a5c62b6b3ef8a5780e8aaf9bbd4093952b796ccd4d9220b8545"
    },
    {
      "path": "tests/test_boris.py",
      "size": 1194,
      "content": "import numpy as np\nfrom pt_sim.forces.boris import integrate_motion_boris\nfrom pt_sim.forces.gauge import Particle\nfrom pt_sim.forces.analytic import helix_radius, cyclotron_frequency\n\ndef uniform_zero_E(t, x): return np.array([0.0,0.0,0.0])\ndef uniform_Bz(Bz):\n    def f(t, x): return np.array([0.0,0.0,Bz])\n    return f\n\ndef test_uniformB_speed_conservation():\n    p = Particle(q=1.0, m=1.0)\n    x0 = (0.0,0.0,0.0); v0 = np.array([0.3, 0.0, 0.0])  # keep sub-relativistic\n    dt = 0.01; steps = 500\n    xs, vs = integrate_motion_boris(p, uniform_zero_E, uniform_Bz(1.0), x0, v0, dt, steps)\n    speeds = np.linalg.norm(vs, axis=1)\n    #assert speeds.ptp() < 1e-3  # tiny variation\n    assert np.ptp(speeds) < 1e-3 \n\ndef test_uniformB_helix_radius():\n    p = Particle(q=2.0, m=1.0)\n    v0 = np.array([0.2, 0.1, 0.0])\n    Bz = 0.7\n    q_over_m = p.q/p.m\n    r_true = helix_radius(np.linalg.norm(v0[:2]), q_over_m, Bz)\n    xs, vs = integrate_motion_boris(p, uniform_zero_E, uniform_Bz(Bz), (0,0,0), v0, dt=0.01, steps=2000)\n    # Estimate radius from xy positions\n    xy = xs[:, :2]\n    rc_est = np.mean(np.linalg.norm(xy - xy.mean(axis=0), axis=1))\n    assert abs(rc_est - r_true)/r_true < 0.05\n",
      "sha256": "c9775fbc7f604a4ebde342bc2df32586893005f0ba54c5608fbdd456693d1fa1"
    },
    {
      "path": "tests/test_detector_bridge.py",
      "size": 358,
      "content": "import numpy as np, json\nfrom pt_sim.detector.bridge import kernel_to_ecal_image\n\ndef test_bridge_image_nonzero(tmp_path):\n    dummy = {\"particles\":[{\"Q\":{\"line\":5,\"pos\":2},\"energy\":3.0}], \"fields\":[{\"energy\":1.5,\"support_edges\":[]}]}\n    img = kernel_to_ecal_image(dummy, n=32, E_scale=1.0)\n    assert img.shape == (32,32)\n    assert float(img.sum()) > 0.0\n",
      "sha256": "9b0838e47b6ae9974023a2ac38f92cf098bbadb888b24124997f1293fe26f759"
    },
    {
      "path": "tests/test_em_shower_energy.py",
      "size": 642,
      "content": "import numpy as np\nfrom pt_sim.physics import simplified as ps\nfrom pt_sim.physics import accurate as pa\n\ndef test_energy_closure_simplified():\n    n = 32; E = 5.0\n    depth = np.linspace(0, 20.0, n)\n    longE = ps.longitudinal_profile(depth, 4.0, 0.3, E)\n    lat = ps.lateral_template(n, 1.2, 1.0)\n    e = sum(ei*lat for ei in longE)\n    assert abs(e.sum()-E)/E < 5e-3\n\ndef test_energy_closure_accurate():\n    n = 32; E = 5.0\n    depth = np.linspace(0, 20.0, n)\n    longE = pa.longitudinal_em(depth, E, X0_cm=1.0, Ec_MeV=10.0)\n    lat = pa.lateral_em(n, 1.0, RM_cm=2.0)\n    e = sum(ei*lat for ei in longE)\n    assert abs(e.sum()-E)/E < 5e-3\n",
      "sha256": "104612b09f89b1858e90a424e662a58b146ba3c3743f6fd0e7e37d0ed551f83c"
    },
    {
      "path": "tests/test_io_stubs.py",
      "size": 625,
      "content": "import numpy as np, os\nfrom pt_sim.io.rootio import write_event_npz, available\nfrom pt_sim.io.hepmc import write_ascii_minimal\n\ndef test_npz_writer(tmp_path):\n    out = tmp_path / \"event.npz\"\n    p = np.array([[1,2,3]])\n    path = write_event_npz(out, {\"p\": p})\n    assert os.path.exists(path)\n\ndef test_hepmc_ascii(tmp_path):\n    out = tmp_path / \"event.hepmc.txt\"\n    parts = [{\"id\": 11, \"px\":0.1, \"py\":0.2, \"pz\":1.0, \"E\":1.05, \"status\":1}]\n    path = write_ascii_minimal(out, parts)\n    assert os.path.exists(path)\n\ndef test_available_returns_dict():\n    info = available()\n    assert \"root\" in info and \"backend\" in info\n",
      "sha256": "816f154f3d50d9b0b6f5c32e9bce7908e649a3aecb487fab953a7894dac51973"
    },
    {
      "path": "tests/test_kernel_emission.py",
      "size": 530,
      "content": "import json, math\nfrom pt_sim.ptk_kernel import PTKKernel, Sound, EmissionConfig\n\ndef test_emission_produces_outputs():\n    ptk = json.load(open(\"ptk.v1.json\"))\n    K = PTKKernel(ptk)\n    pos = [Sound(\"n1\", +1, 5.0, 440.0, 0.0, 0.9)]\n    neg = [Sound(\"n4\", -1, 5.0, 440.0, math.pi, 0.9)]\n    res = K.simulate_emission(pos, neg, EmissionConfig(max_steps=20))\n    assert (\"particles\" in res) and (\"fields\" in res)\n    # At least *something* happens in this configuration\n    assert (len(res[\"particles\"]) + len(res[\"fields\"])) >= 1\n",
      "sha256": "5fd297dec533ba809e8c7ffa5aa8ee43343a8dbf5203dcc0f6e2fa8f9d049e29"
    },
    {
      "path": "tests/test_kernel_wiring.py",
      "size": 726,
      "content": "import json, math, os\nfrom pt_sim.ptk_kernel import PTKKernel, Sound, EmissionConfig\nfrom pt_sim.detector.bridge import kernel_to_ecal_image\n\ndef test_kernel_to_detector_roundtrip(tmp_path):\n    # Load kernel\n    ptk = json.load(open(\"ptk.v1.json\", \"r\", encoding=\"utf-8\"))\n    K = PTKKernel(ptk)\n    # Emit\n    pos = [Sound(\"n1\", +1, 5.0, 440.0, 0.0, 0.9, \"P1\")]\n    neg = [Sound(\"n4\", -1, 5.0, 440.0, 3.14159, 0.9, \"N1\")]\n    res = K.simulate_emission(pos, neg, EmissionConfig(max_steps=30))\n    assert \"particles\" in res and \"fields\" in res and \"detections\" in res\n    # Detect (image must be non-zero)\n    img = kernel_to_ecal_image(res, n=32, e_scale=1.0)\n    assert img.shape == (32,32)\n    assert float(img.sum()) > 0.0\n",
      "sha256": "977ad9b6fc2329136375717e2340e1bb3a34de4096dce779f7835bc1f50401ac"
    },
    {
      "path": "tools/run_emission.py",
      "size": 1397,
      "content": "#!/usr/bin/env python\nimport os, json, math, argparse\nfrom pt_sim.ptk_kernel import PTKKernel, Sound, EmissionConfig\n\ndef load_sounds(slist):\n    out = []\n    for s in slist:\n        out.append(Sound(\n            node_id=s[\"node_id\"], polarity=int(s[\"polarity\"]),\n            energy=float(s[\"energy\"]), frequency=float(s[\"frequency\"]),\n            phase=float(s.get(\"phase\", 0.0)), coherence=float(s.get(\"coherence\", 1.0)),\n            label=s.get(\"label\",\"\")\n        ))\n    return out\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--kernel\", default=\"ptk.v1.json\", help=\"Path to ptk.v1.json\")\n    ap.add_argument(\"--positive\", required=True, help=\"JSON list of + sounds\")\n    ap.add_argument(\"--negative\", required=True, help=\"JSON list of - sounds\")\n    ap.add_argument(\"--cfg\", default=\"{}\", help=\"EmissionConfig JSON\")\n    ap.add_argument(\"--out\", default=\"out/ptk_emission_result.json\")\n    args = ap.parse_args()\n\n    ptk = json.load(open(args.kernel, \"r\", encoding=\"utf-8\"))\n    K = PTKKernel(ptk)\n    pos = load_sounds(json.loads(args.positive))\n    neg = load_sounds(json.loads(args.negative))\n    cfg = EmissionConfig(**json.loads(args.cfg))\n\n    res = K.simulate_emission(pos, neg, cfg)\n    os.makedirs(os.path.dirname(args.out), exist_ok=True)\n    json.dump(res, open(args.out, \"w\"), indent=2)\n    print(\"Wrote\", args.out)\n\nif __name__ == \"__main__\":\n    main()\n",
      "sha256": "4a98886610c787ed1ccf18473d7763ae9eb1509150c7500210417105735865c3"
    },
    {
      "path": "tools/viz_overlay.py",
      "size": 1826,
      "content": "import os, json, matplotlib.pyplot as plt\n\ndef main(ptk_path=\"ptk.v1.json\", layout_path=\"ptk.layout.json\", emission_json=\"out/ptk_emission_result.json\", out=\"out/ptk_overlay.png\"):\n    ptk = json.load(open(ptk_path, \"r\", encoding=\"utf-8\"))\n    layout = json.load(open(layout_path, \"r\", encoding=\"utf-8\"))\n    coords = layout[\"coords\"]\n    labels = {n[\"id\"]: n[\"label\"] for n in ptk[\"nodes\"]}\n\n    fig, ax = plt.subplots(figsize=(14,10))\n    ax.set_aspect('equal'); ax.axis('off')\n\n    # Edges\n    for e in ptk[\"edges\"]:\n        x1,y1 = coords[e[\"source\"]]; x2,y2 = coords[e[\"target\"]]\n        color = \"green\" if e[\"polarity\"]==1 else \"red\"\n        ax.plot([x1,x2],[y1,y2], color=color, lw=0.6, alpha=0.35)\n\n    # Nodes\n    for nid,(x,y) in coords.items():\n        ax.plot(x,y,\"o\", color=\"white\", mec=\"black\", ms=4, zorder=3)\n        ax.text(x,y,labels.get(nid,\"\"), fontsize=7, ha=\"center\", va=\"center\", zorder=4)\n\n    # Emission overlay (if present)\n    if os.path.exists(emission_json):\n        res = json.load(open(emission_json))\n        # Fields: thicken support edges\n        support = set(eid for f in res.get(\"fields\", []) for eid in f.get(\"support_edges\", []))\n        for e in ptk[\"edges\"]:\n            if e[\"id\"] in support:\n                x1,y1 = coords[e[\"source\"]]; x2,y2 = coords[e[\"target\"]]\n                ax.plot([x1,x2],[y1,y2], lw=2.0, alpha=0.4, color=\"royalblue\")\n        # Particles: stars on nodes\n        for p in res.get(\"particles\", []):\n            x,y = coords[p[\"locus\"]]\n            ax.plot(x,y, marker=\"*\", ms=14, mec=\"black\", mfc=\"gold\", zorder=5)\n\n    plt.title(\"Param Tatva — Positive (green), Negative (red), Fields (blue), Particles (gold)\")\n    os.makedirs(os.path.dirname(out), exist_ok=True)\n    plt.savefig(out, dpi=150)\n    print(\"Wrote\", out)\n\nif __name__ == \"__main__\":\n    main()\n",
      "sha256": "cb2c29a447bd027556cbed24d5b88037228cd021b83a9bf34ea7969e06a65508"
    }
  ]
}